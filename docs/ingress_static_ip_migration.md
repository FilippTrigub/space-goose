# Shared Ingress Migration Notes

## Background
- Each project currently provisions a distinct `LoadBalancer` Service, which causes slow or failed public-IP assignments when pods churn.
- Move to a single ingress layer with a reserved static IP so pods only need `ClusterIP` Services and HTTP routing is managed centrally.

## Cluster Changes (Commands)
- Make sure your kubectl context targets the AKS cluster and capture the node resource group (the place where public IPs live).

```sh
az aks get-credentials --resource-group <aks-resource-group> --name <aks-cluster-name> --overwrite-existing

for /f "delims=" %I in ( ^
  'az aks show --resource-group <aks-resource-group> --name <aks-cluster-name> --query "nodeResourceGroup" -o tsv' ^
) do set RESOURCE_GROUP=%I

set LOCATION=<azure-region>        REM example: eastus
```

Note: in my case
```sh
set RESOURCE_GROUP=MC_goose-legion_group_goose-legion_francecentral
set LOCATION=francecentral
```

- Reserve a Standard SKU static public IP and (optionally) an Azure-provided DNS label. Replace the placeholders before running these commands in Windows Command Prompt.

```sh
set DNS_LABEL=goose-%RANDOM%       REM set to blank ("set DNS_LABEL=") if you do not want Azure to host DNS

if defined DNS_LABEL (
  az network public-ip create ^
    --resource-group %RESOURCE_GROUP% ^
    --name goose-static-ingress ^
    --sku Standard ^
    --allocation-method static ^
    --dns-name %DNS_LABEL% ^
    --location %LOCATION%
) else (
  az network public-ip create ^
    --resource-group %RESOURCE_GROUP% ^
    --name goose-static-ingress ^
    --sku Standard ^
    --allocation-method static ^
    --location %LOCATION%
)

for /f "delims=" %I in ( ^
  'az network public-ip show --resource-group %RESOURCE_GROUP% --name goose-static-ingress --query "ipAddress" -o tsv' ^
) do set STATIC_INGRESS_IP=%I

if defined DNS_LABEL (
  set PUBLIC_APP_DOMAIN=%DNS_LABEL%.%LOCATION%.cloudapp.azure.com
) else (
  set PUBLIC_APP_DOMAIN=<your-domain.example.com>
)

set INGRESS_CLASS_NAME=nginx

echo Static ingress IP: %STATIC_INGRESS_IP%
echo Public app domain: %PUBLIC_APP_DOMAIN%
```

> **Note:** `PUBLIC_APP_DOMAIN` must always be non-empty for the Python service. If you rely on Azureâ€™s autogenerated host, keep `DNS_LABEL` defined and use the resulting `*.cloudapp.azure.com` value here. Otherwise provide a domain you control and point its DNS records at the static IP.

- Deploy (or update) the ingress controller. Use the upstream ingress-nginx manifest first (it installs the required RBAC pieces), then patch the Service to pin your static IP and health-probe annotation.

```sh
kubectl apply -f https://raw.githubusercontent.com/kubernetes/ingress-nginx/main/deploy/static/provider/cloud/deploy.yaml

kubectl patch service ingress-nginx-controller -n ingress-nginx ^
  --type merge ^
  --patch "{\"spec\":{\"loadBalancerIP\":\"%STATIC_INGRESS_IP%\",\"externalTrafficPolicy\":\"Local\"},\"metadata\":{\"annotations\":{\"service.beta.kubernetes.io/azure-load-balancer-health-probe-request-path\":\"/healthz\"}}}"
```

- If you prefer to manage the manifest locally, the following block mirrors the syntax used in `docs\KUBERNETES_SETUP.md` for Command Prompt. 
- SKIP THIS if you rely on the upstream manifest above.

```sh
if not exist infra mkdir infra

(
echo apiVersion: v1
echo kind: Namespace
echo metadata:
echo ^  name: ingress-nginx
echo ---
echo apiVersion: apps/v1
echo kind: Deployment
echo metadata:
echo ^  name: ingress-nginx-controller
echo ^  namespace: ingress-nginx
echo spec:
echo ^  replicas: 2
echo ^  selector:
echo ^    matchLabels:
echo ^      app.kubernetes.io/name: ingress-nginx
echo ^      app.kubernetes.io/component: controller
echo ^  template:
echo ^    metadata:
echo ^      labels:
echo ^        app.kubernetes.io/name: ingress-nginx
echo ^        app.kubernetes.io/component: controller
echo ^    spec:
echo ^      serviceAccountName: ingress-nginx
echo ^      containers:
echo ^      - name: controller
echo ^        image: registry.k8s.io/ingress-nginx/controller:v1.11.1
echo ^        args:
echo ^        - /nginx-ingress-controller
echo ^        - --publish-service=^$(POD_NAMESPACE^)/ingress-nginx-controller
echo ^        - --ingress-class=nginx
echo ^        readinessProbe:
echo ^          httpGet:
echo ^            path: /healthz
echo ^            port: 10254
echo ^        livenessProbe:
echo ^          httpGet:
echo ^            path: /healthz
echo ^            port: 10254
echo ---
echo apiVersion: v1
echo kind: Service
echo metadata:
echo ^  name: ingress-nginx-controller
echo ^  namespace: ingress-nginx
echo ^  annotations:
echo ^    service.beta.kubernetes.io/azure-load-balancer-health-probe-request-path: /healthz
echo spec:
echo ^  type: LoadBalancer
echo ^  loadBalancerIP: %STATIC_INGRESS_IP%
echo ^  externalTrafficPolicy: Local
echo ^  ports:
echo ^  - name: http
echo ^    port: 80
echo ^    targetPort: 80
echo ^  - name: https
echo ^    port: 443
echo ^    targetPort: 443
echo ^  selector:
echo ^    app.kubernetes.io/name: ingress-nginx
echo ^    app.kubernetes.io/component: controller
) > ingress-nginx.yaml

kubectl apply -f ingress-nginx.yaml
```

- For each project namespace, tear down the old per-project load balancer Service, then generate the ClusterIP Service and the Ingress. Set the variables first so the host is built consistently with the Python logic.

```sh
set USER_ID=user2
set PROJECT_ID=test
set PROJECT_HOST=%PROJECT_ID%-%USER_ID%.%PUBLIC_APP_DOMAIN%

if not exist environments mkdir environments
if not exist environments\user-%USER_ID% mkdir environments\user-%USER_ID%
if not exist environments\user-%USER_ID%\proj-%PROJECT_ID% mkdir environments\user-%USER_ID%\proj-%PROJECT_ID%

REM Scale down the deployment and remove the old LoadBalancer service
kubectl scale deployment proj-%PROJECT_ID%-api -n user-%USER_ID% --replicas=0
kubectl delete service proj-%PROJECT_ID%-api -n user-%USER_ID% --ignore-not-found

(
echo apiVersion: v1
echo kind: Service
echo metadata:
echo ^  name: proj-%PROJECT_ID%-api
echo ^  namespace: user-%USER_ID%
echo ^  labels:
echo ^    app: proj-%PROJECT_ID%-api
echo ^    project-id: %PROJECT_ID%
echo ^    user-id: %USER_ID%
echo spec:
echo ^  selector:
echo ^    app: proj-%PROJECT_ID%-api
echo ^  ports:
echo ^  - name: http
echo ^    port: 80
echo ^    targetPort: 3001
echo ^  type: ClusterIP
) > environments\user-%USER_ID%\proj-%PROJECT_ID%\service.yaml

(
echo apiVersion: networking.k8s.io/v1
echo kind: Ingress
echo metadata:
echo ^  name: proj-%PROJECT_ID%-api
echo ^  namespace: user-%USER_ID%
echo ^  annotations:
echo ^    kubernetes.io/ingress.class: %INGRESS_CLASS_NAME%
echo spec:
echo ^  rules:
echo ^  - host: %PROJECT_HOST%
echo ^    http:
echo ^      paths:
echo ^      - path: /
echo ^        pathType: Prefix
echo ^        backend:
echo ^          service:
echo ^            name: proj-%PROJECT_ID%-api
echo ^            port:
echo ^              number: 80
) > environments\user-%USER_ID%\proj-%PROJECT_ID%\ingress.yaml

REM Append a TLS section manually if you manage certificates for %PROJECT_HOST%

kubectl apply -f environments\user-%USER_ID%\proj-%PROJECT_ID%\service.yaml
kubectl apply -f environments\user-%USER_ID%\proj-%PROJECT_ID%\ingress.yaml

REM Scale the deployment back up once the ingress is in place
kubectl scale deployment proj-%PROJECT_ID%-api -n user-%USER_ID% --replicas=1
```

## Python Service Orchestration Updates (`k8s_manager/services/k8s_service.py`)
- `apply_project_resources`:
  - Change the generated Service to `type="ClusterIP"` and remove per-port Azure load-balancer annotations.
  - After creating the Service, create or patch a matching Ingress resource using the host pattern above; ensure host/domain comes from configuration (e.g., `PUBLIC_APP_DOMAIN`).
  - Inject TLS secret name only when certificate automation is enabled; otherwise skip the TLS block.
- `wait_for_loadbalancer_ip`:
  - Poll `NetworkingV1Api.read_namespaced_ingress` until `.status.loadBalancer.ingress[0].ip|hostname` is set.
  - Return both the routed host and the controller's IP/hostname so health checks can hit the static IP with the correct `Host` header.
- `get_project_endpoint`:
  - Return both the project host and the controller IP/hostname (`{"host", "ip", "lb_hostname"}`) so callers can send requests to the shared ingress IP with the correct `Host` header.
- `delete_project_resources`:
  - Add deletion of the `Ingress` object prior to Service removal to avoid stale hosts.
- Configuration:
  - Introduce a required env var for the public domain suffix and optionally one for the ingress class name.
  - Update logs/error messages to reference ingress readiness rather than load balancer provisioning.

## Follow-Up
- Run smoke tests (external curl) against the new ingress endpoint after deployment.
- Update any DNS automation to create host records that map to the static IP before first traffic hits the ingress controller.
- Set the environment variables (`PUBLIC_APP_DOMAIN`, `INGRESS_CLASS_NAME`, optional `INGRESS_TLS_SECRET_PATTERN`) everywhere the Python service runs; re-source them before restarting `k8s_manager`.
- Re-run the `envsubst ... | kubectl apply -f -` commands whenever you change controller images, static IPs, or namespace defaults.
